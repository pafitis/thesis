{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('comp0087': conda)"
  },
  "interpreter": {
   "hash": "6b9679e1c6c858221750b0560d352a46169916bc602dce44bbd2a48d631c8c5d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# June 14th Meeting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Intro\n",
    "\n",
    "I completed my exams! I took a much needed break, celebrated my 25th birthday and have started working on my dissertation. I am applying for jobs too and that was somewhat a time sink... hopefully they'll like me!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Reading\n",
    "1. Understanding tables with intermediate pre-training (Eisenchlos et al, https://arxiv.org/abs/2010.00571)\n",
    "2. Joint verification and reranking for open fact checking over tables (Riedel et al, https://arxiv.org/abs/2012.15115)\n",
    "3. Logical Natural Language Generation from Open-Domain Tables (Chen et al, https://arxiv.org/abs/2004.10404)\n",
    "4. Logic2Text: High-Fidelity Natural Language Generation from Logical Forms (Chen et al, https://arxiv.org/abs/2004.14579)\n",
    "5. TAPAS: Reasoning over tables with intermediate pre-training (Muller, Eisenchlos et al, https://arxiv.org/abs/2104.01099)\n",
    "6. Open Question Answering over Tables and Text (Chen et al, https://openreview.net/forum?id=MmCRswl1UYl)\n",
    "7. Open domain QA over tables via dense retrieval (Herzig, Eisenchlos et al, https://arxiv.org/abs/2103.12011)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Coding\n",
    "\n",
    "1. I've been reading the API documentation and also contacted ONS\n",
    "    - The documentation isn't particularly thorough; I am getting the hang of it by trial and error\n",
    "    - I did obtain a contact email for questions but they are not responsive, as expected\n",
    "\n",
    "2. I wrote a quick scraper that can parse bulletins\n",
    "    - requests, beautifulsoup, parses and stores in a 'database' (dictionaries)\n",
    "    - I extract: title, links, relevant publications, relevant datasets, metadata\n",
    "    - I took the liberty to perform some preprocessing\n",
    "        - Removed `|` character, artifact of how contact details are stored on the page\n",
    "        - Removed references to Twitter or to the ONS (these were referrals for reader to check-out other socials)\n",
    "        - **There are sentences about rounding errors, how averages are calculated. I started writing catches to remove them but I am not sure how it will scale**\n",
    "        - Removed whitespaces and urls\n",
    "\n",
    "3. Reading the *relevant publications* requires some further processing which I started writing\n",
    "    - I am scraping text found in both `<li>` and `<p>` html tags\n",
    "    - Those in list form, are sentences in a usable format without much preprocessing required <img src=\"images/june14_mainpoints.png\" width=\"500\">\n",
    "    - Those in paragraph form, require us to design a scalable solution to generate the queries <img src=\"images/june14_paragraph.png\" width=\"500\">\n",
    "    - When (or if) should I focus on writing a solution to extract queries from the paragraph style text?\n",
    "        - Difficulty would be in identifying what senteces we should drop?\n",
    "\n",
    "4. Reading the *relevant datasets* you quickly observe that style and format is inconsistent\n",
    "    - Initially I thought this was going to be a bigger problem\n",
    "    - But reading the papers I see that **Table Linearization** should be sufficient, assuming we can identify WHERE the relevant data starts\n",
    "    - Tables don't necessarily start on the same row number, so you need to identify the correct position.\n",
    "    - (The API can be used to query specific observations/ variables so we can **possibly** utilise that approach?)\n",
    "    - What if table has multi-level column-headers? Would Linearization be consistent for tables that don't have this?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Key Observations\n",
    "\n",
    "1. The API does not provide a method to find publications from datasets. You need to read the website and scrape the publications, then you need to call the API and find the dataset with some particular ID.\n",
    "    - I need to understand how to link the publications with the datasets.\n",
    "    - If this isn't done right it will surely come back to bite me...\n",
    "    - I have some experience building a retrieval indexer, maybe I should take that approach?\n",
    "        - Retrieve a number of publications\n",
    "        - Find the relevant datasets that are used\n",
    "        - Find their IDs and store them in an index\n",
    "        - You would then call the publication and it would provide a dataset ID you can pass to the API and progress?\n",
    "    - Thoughts?\n",
    "\n",
    "2. Would you argue that sentences from the \"Main Points\" list style (example above) are sufficient for our training data? We can use those to do our counterfactual generation.\n",
    "    - This relies on the ONS website design purely. It does not scale to other websites, what happens if they decide to change it?\n",
    "    - Are these issues I need to have in mind, in terms of how generalisable my research is?\n",
    "    - If we want to use the paragraph style sentences, then I need to understand how I can remove sentences that do not refer to a dataset/ table. (see below, the publications are full of these)\n",
    "        - *\"Following a classification review, we concluded that the ERMAs placed TOCs under public sector control.\"*\n",
    "\n",
    "\n",
    "3. Would it make sense to start by focusing on sentences that contain numerical statements? I can think of some 'hacky' ways to generate a training dataset fast. And we wouldn't need to work with counterfactual data generation\n",
    "    - Eisenchlos et al. utilise other rows in the table to \"scramble\" and generate counterfactuals\n",
    "    - Xiong et al. go the MASK approach and switch words of the same grammar family"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Connect publications to datasets\n",
    "2. Understand and implement preprocessing needed to get at a **clean** training set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}