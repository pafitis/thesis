{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tapas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, logging, spacy, sys, os, json, requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers.classes import Collection\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from helpers.cloze_generation import generate_clozes_from_point, named_entity_answer_generator as ne_answer_generator, noun_phrase_answer_generator as np_answer_generator\n",
    "\n",
    "from helpers.table_processing import preprocess_table\n",
    "\n",
    "from helpers.language_modelling import run_language_model, summarise_results\n",
    "\n",
    "df = pd.read_pickle('pickles/dataset_20210625_184837.pkl')\n",
    "clozes_df = pd.read_json('pickles/clozes_20210715_212425.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering, TapasForMaskedLM\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def replace_mask(\n",
    "    sentence, \n",
    "    masks = ['IDENTITYMASK', 'NOUNPHRASEMASK', 'NUMERICMASK', \n",
    "        'PLACEMASK', 'TEMPORALMASK', 'THINGMASK']):\n",
    "\n",
    "    # somewhat hacky\n",
    "    # checks if sentence contains any of the masks\n",
    "    # and replaces it with the appropriate tokenizer.mask_token\n",
    "    x = [sentence.replace(x, tokenizer.mask_token) \\\n",
    "        for x in masks if x in sentence]\n",
    "    if len(x):\n",
    "        return x[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model_name = 'google/tapas-base-finetuned-wtq'\n",
    "# model = TapasForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = TapasTokenizer.from_pretrained(model_name)\n",
    "model = TapasForMaskedLM.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TapasForMaskedLM: ['output_weights', 'column_output_weights', 'output_bias', 'column_output_bias', 'aggregation_classifier.weight', 'aggregation_classifier.bias', 'tapas.pooler.dense.weight', 'tapas.pooler.dense.bias']\n",
      "- This IS expected if you are initializing TapasForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TapasForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TapasForMaskedLM were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "row = df.iloc[0]\n",
    "point = row.point\n",
    "datasets = row.data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "clozes = [c for c in generate_clozes_from_point(point, ne_answer_generator)]\n",
    "# queries = [replace_mask(c.cloze_text) for c in clozes]\n",
    "\n",
    "# queries = [\n",
    "#     'What was the approximate gross value added of the UK non-financial business economy in 2019?',\n",
    "#     'What was the approximate gross value added of the UK non-financial business economy in 2018?',\n",
    "#     'Was the approximate gross value added of the UK non-financial business economy higher in 2019 than 2018?',\n",
    "#     'Which year had the highest total turnover?',\n",
    "#     'Which year had the lowest total turnover?']\n",
    "\n",
    "queries = [\n",
    "    f'The approximate gross value added of the UK non-financial business in 2019 was {tokenizer.mask_token}',\n",
    "    f'The year with the highest total turnover was {tokenizer.mask_token}'\n",
    "    ]\n",
    "\n",
    "data = datasets[2].replace('/', '_')[1:]\n",
    "excel_file = pd.ExcelFile('datasets/' + data + '.xls')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "excel_file.sheet_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Contents',\n",
       " 'Non-Financial Business Economy',\n",
       " 'Section-Division by Region',\n",
       " 'Region by Section-Division']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data = excel_file.parse('Non-Financial Business Economy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "table = preprocess_table(data).astype(str)\n",
    "table = table[175:181].reset_index(drop = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "table"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Standard Industrial Classification (Revised 2007)     Section  \\\n",
       "0                                       A-S (Part) 2              \n",
       "1                                       A-S (Part) 2              \n",
       "2                                       A-S (Part) 2              \n",
       "3                                       A-S (Part) 2              \n",
       "4                                       A-S (Part) 2              \n",
       "5                                       A-S (Part) 2              \n",
       "\n",
       "                                   - Country and Region  Year Total turnover  \\\n",
       "0  UK non-financial business economy     United Kingdom  2014        3445024   \n",
       "1  UK non-financial business economy     United Kingdom  2015        3365823   \n",
       "2  UK non-financial business economy     United Kingdom  2016        3525775   \n",
       "3  UK non-financial business economy     United Kingdom  2017        3823162   \n",
       "4  UK non-financial business economy     United Kingdom  2018        4029100   \n",
       "5  UK non-financial business economy     United Kingdom  2019        4099272   \n",
       "\n",
       "  Approximate gross value added at basic prices (aGVA)  \\\n",
       "0                                            1087175     \n",
       "1                                            1142070     \n",
       "2                                            1161990     \n",
       "3                                            1218978     \n",
       "4                                            1269185     \n",
       "5                                            1311079     \n",
       "\n",
       "  Total purchases of goods, materials and services Total employment costs  \n",
       "0                                          2337331                 564856  \n",
       "1                                          2200721                 591906  \n",
       "2                                          2338100                 616451  \n",
       "3                                          2582797                 642618  \n",
       "4                                          2735518                 661686  \n",
       "5                                          2761846                 697774  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard Industrial Classification (Revised 2007)     Section</th>\n",
       "      <th>-</th>\n",
       "      <th>Country and Region</th>\n",
       "      <th>Year</th>\n",
       "      <th>Total turnover</th>\n",
       "      <th>Approximate gross value added at basic prices (aGVA)</th>\n",
       "      <th>Total purchases of goods, materials and services</th>\n",
       "      <th>Total employment costs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-S (Part) 2</td>\n",
       "      <td>UK non-financial business economy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2014</td>\n",
       "      <td>3445024</td>\n",
       "      <td>1087175</td>\n",
       "      <td>2337331</td>\n",
       "      <td>564856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-S (Part) 2</td>\n",
       "      <td>UK non-financial business economy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015</td>\n",
       "      <td>3365823</td>\n",
       "      <td>1142070</td>\n",
       "      <td>2200721</td>\n",
       "      <td>591906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-S (Part) 2</td>\n",
       "      <td>UK non-financial business economy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2016</td>\n",
       "      <td>3525775</td>\n",
       "      <td>1161990</td>\n",
       "      <td>2338100</td>\n",
       "      <td>616451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-S (Part) 2</td>\n",
       "      <td>UK non-financial business economy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2017</td>\n",
       "      <td>3823162</td>\n",
       "      <td>1218978</td>\n",
       "      <td>2582797</td>\n",
       "      <td>642618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-S (Part) 2</td>\n",
       "      <td>UK non-financial business economy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2018</td>\n",
       "      <td>4029100</td>\n",
       "      <td>1269185</td>\n",
       "      <td>2735518</td>\n",
       "      <td>661686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A-S (Part) 2</td>\n",
       "      <td>UK non-financial business economy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2019</td>\n",
       "      <td>4099272</td>\n",
       "      <td>1311079</td>\n",
       "      <td>2761846</td>\n",
       "      <td>697774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "inputs = tokenizer(\n",
    "    table=table, queries=queries[0], padding='max_length', return_tensors='pt',\n",
    "    truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(\n",
    "#     inputs,\n",
    "#     outputs.logits.detach(),\n",
    "#     outputs.logits_aggregation.detach())\n",
    "\n",
    "\n",
    "# predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(\n",
    "#     inputs, outputs.logits.detach())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sentence = queries[0]\n",
    "verbose = True\n",
    "_iter = 0\n",
    "sequence_confidence = 0\n",
    "sequence_confidences = []\n",
    "\n",
    "answer_given = []\n",
    "\n",
    "top_k_vocab = 1\n",
    "# find where masks are located\n",
    "is_masked = torch.where(\n",
    "    inputs.input_ids == tokenizer.mask_token_id, 1, 0\n",
    ")\n",
    "masked_idxs = torch.nonzero(is_masked)\n",
    "\n",
    "# convert to probabilities\n",
    "probabilities = torch.softmax(\n",
    "    outputs.logits[0, masked_idxs[:, 1]], dim = 1)\n",
    "logprobs = torch.log(probabilities)\n",
    "\n",
    "# obtain k most confident token predictions, work on logprobs to avoid underflow\n",
    "mask_confidence, token_ids = torch.topk(logprobs, top_k_vocab)\n",
    "\n",
    "# selects the mask index that correspond to the most confident prediction; I am slicing [0] because of top_k_vocab will return the k most confident possible tokens. ultimately top_k_vocab is not used, but I am keeping it here for future work\n",
    "most_confident = mask_confidence.argmax(dim = 0)[0].item()\n",
    "target_token_idx = token_ids[most_confident][0]\n",
    "target_token = tokenizer.decode(target_token_idx)\n",
    "\n",
    "# confidence as a proxy of probability\n",
    "token_confidence = mask_confidence[most_confident][0].item()\n",
    "\n",
    "# add logprobabilities to obtain sequence probability\n",
    "sequence_confidence += token_confidence\n",
    "sequence_confidences.append(token_confidence)\n",
    "\n",
    "# find start and end index of <mask> to be removed\n",
    "starting_pos = find_nth_substring(\n",
    "    sentence, tokenizer.mask_token, most_confident)\n",
    "ending_pos = starting_pos + len(tokenizer.mask_token)\n",
    "\n",
    "# construct new version of sentence\n",
    "# replace mask by predicted token\n",
    "sentence = sentence[:starting_pos] + \\\n",
    "    target_token + sentence[ending_pos:]\n",
    "\n",
    "# answer_given = [ (token, position), ... ]\n",
    "answer_given.append((target_token, starting_pos))\n",
    "\n",
    "if verbose:\n",
    "    print(f'Iteration: {_iter}, Predicted Token: {target_token}, Iteration Confidence: {token_confidence}, Confidence (sequence): {sequence_confidence}')\n",
    "    print(f'Sentence: {sentence}')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-50cbb42cd7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtop_k_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# find where masks are located\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m is_masked = torch.where(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from transformers import TapasTokenizer, TapasForMaskedLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "tokenizer = TapasTokenizer.from_pretrained('google/tapas-base')\n",
    "model = TapasForMaskedLM.from_pretrained('google/tapas-base')\n",
    "\n",
    "data = {'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"],\n",
    "        'Age': [\"56\", \"45\", \"59\"],\n",
    "        'Number of movies': [\"87\", \"53\", \"69\"]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "inputs = tokenizer(table=table, queries=\"How many [MASK] has George [MASK] played in?\", return_tensors=\"pt\")\n",
    "labels = tokenizer(table=table, queries=\"How many movies has George Clooney played in?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at google/tapas-base were not used when initializing TapasForMaskedLM: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing TapasForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TapasForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TapasForMaskedLM were not initialized from the model checkpoint at google/tapas-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (34).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-49d68fd38fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"How many movies has George Clooney played in?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/comp0087/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/comp0087/lib/python3.9/site-packages/transformers/models/tapas/modeling_tapas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# -100 index = padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/comp0087/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/comp0087/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/comp0087/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/comp0087/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2382\u001b[0m             \u001b[0;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (34)."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "outputs = model(**inputs, labels = labels[0][1:-1])\n",
    "last_hidden_states = outputs.last_hidden_state"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'last_hidden_state'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a78d46a55c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "labels"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  101,  2129,  2116,  5691,  2038,  2577, 18856,  7828,  3240,  2209,\n",
       "          1999,  1029,   102,  5889,  2287,  2193,  1997,  5691,  8226, 15091,\n",
       "          5179,  6584, 14720,  4487,  6178,  9488,  3429,  5187,  2577, 18856,\n",
       "          7828,  3240,  5354,  6353]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from transformers import TapasConfig,TapasTokenizer,TapasForMaskedLM\n",
    "import torch\n",
    "config = TapasConfig.from_pretrained('google/tapas-base-finetuned-wtq',from_pt=True)\n",
    "model = TapasForMaskedLM.from_pretrained('google/tapas-base-finetuned-wtq', config=config)\n",
    "tokenizer=TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\",from_pt=True)\n",
    "import sys\n",
    "\n",
    "# outdir = \"tmp\"\n",
    "\n",
    "# model.save_pretrained(outdir)\n",
    "# tokenizer.save_pretrained(outdir)\n",
    "# config.save_pretrained(outdir)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(task=\"fill-mask\",framework=\"pt\",model=model, tokenizer=tokenizer)\n",
    "#nlp = pipeline(task=\"table-question-answering\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data= {    \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n",
    "    \"age\": [\"56\", \"45\", \"59\"],\n",
    "    \"number of movies\": [\"87\", \"53\", \"69\"],\n",
    "    \"date of birth\": [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"]\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TapasForMaskedLM: ['output_weights', 'column_output_weights', 'output_bias', 'column_output_bias', 'aggregation_classifier.weight', 'aggregation_classifier.bias', 'tapas.pooler.dense.weight', 'tapas.pooler.dense.bias']\n",
      "- This IS expected if you are initializing TapasForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TapasForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TapasForMaskedLM were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import numpy as np\n",
    "table = pd.DataFrame.from_dict(data)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "queries=[\n",
    "        f\"The number of movies Brad Pitt acted in is {tokenizer.mask_token}\",\n",
    "        f\"Leonardo di caprio's age is {tokenizer.mask_token}\"]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('comp0087': conda)"
  },
  "interpreter": {
   "hash": "6b9679e1c6c858221750b0560d352a46169916bc602dce44bbd2a48d631c8c5d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}