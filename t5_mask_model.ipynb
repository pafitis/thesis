{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# T5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, logging, spacy, sys, os, json, requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers.classes import Collection\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from helpers.cloze_generation import generate_clozes_from_point, named_entity_answer_generator as ne_answer_generator, noun_phrase_answer_generator as np_answer_generator\n",
    "\n",
    "from helpers.table_processing import preprocess_table\n",
    "\n",
    "from helpers.language_modelling import run_language_model, summarise_results\n",
    "\n",
    "df = pd.read_pickle('pickles/dataset_20210625_184837.pkl')\n",
    "clozes_df = pd.read_json('pickles/clozes_20210715_212425.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def replace_mask(\n",
    "    sentence, \n",
    "    masks = ['IDENTITYMASK', 'NOUNPHRASEMASK', 'NUMERICMASK', \n",
    "        'PLACEMASK', 'TEMPORALMASK', 'THINGMASK']):\n",
    "\n",
    "    # somewhat hacky\n",
    "    # checks if sentence contains any of the masks\n",
    "    # and replaces it with the appropriate tokenizer.mask_token\n",
    "    x = [sentence.replace(x, tokenizer.mask_token) \\\n",
    "        for x in masks if x in sentence]\n",
    "    if len(x):\n",
    "        return x[0]\n",
    "\n",
    "\n",
    "def find_nth_substring(sentence, substring, n):\n",
    "    '''\n",
    "    used internally in multitoken_prediction to find the n-th occurence of a specific substring in a sentence\n",
    "    returns the starting index of substring\n",
    "    '''\n",
    "    start = sentence.find(substring)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = sentence.find(substring, start+len(substring))\n",
    "        n -= 1\n",
    "    return start"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model_name = 't5-base'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "config = T5Config.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config = config)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, config = config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# text = 'Brad Pitt is <extra_id_0> years old. Row 1: Actor is Brad Pitt; Age is 56; Number of movies is 87. Row 2: Actor is George Clooney; Age is 59; Number of movies is 69.'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "text = 'Byron got ignored by <extra_id_0>. Row 1: Person is Byron; Date is Eleni; Failed is Doris. Row 2: Person is Savvas; Date is Pavlos; Failed is Byron'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "text = \"Pavlos really hates <extra_id_0>. Row 1: User is Pavlos; Enemy: Andrestinos. Row 2: User is Savvas; Enemy is Matsis\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "encoded = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
    "input_ids = encoded['input_ids']\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    num_beams=10, num_return_sequences=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "_0_index = text.index('<extra_id_0>')\n",
    "_result_prefix = text[:_0_index]\n",
    "_result_suffix = text[_0_index+12:]  # 12 is the length of <extra_id_0>\n",
    "\n",
    "def _filter(output, end_token='<extra_id_1>'):\n",
    "    # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
    "    _txt = tokenizer.decode(\n",
    "        output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    if end_token in _txt:\n",
    "        _end_token_index = _txt.index(end_token)\n",
    "        return _result_prefix + _txt[:_end_token_index] + _result_suffix\n",
    "    else:\n",
    "        return _result_prefix + _txt + _result_suffix\n",
    "\n",
    "results = list(map(_filter, outputs))\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Byron got ignored by Doris. Row 1: Person is Byron; Date is Eleni; Failed is Doris. Row 2: Person is Savvas; Date is Pavlos; Failed is Byron',\n",
       " 'Byron got ignored by Doris. Row 1: Person is Byron; Date is Eleni; Failed is Doris. Row 2: Person is Savvas; Date is Pavlos; Failed is Byron',\n",
       " 'Byron got ignored by Doris. Row 1: Person is Byron; Date is Eleni; Failed is Doris. Row 2: Person is Savvas; Date is Pavlos; Failed is Byron',\n",
       " 'Byron got ignored by Doris. Row 1: Person is Byron; Date is Eleni; Failed is Doris. Row 2: Person is Savvas; Date is Pavlos; Failed is Byron',\n",
       " 'Byron got ignored by Doris. Row 1: Person is Byron; Date is Eleni; Failed is Doris. Row 2: Person is Savvas; Date is Pavlos; Failed is Byron']"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "outputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[    0, 32099,  6200,   159, 32098,  6200,   159, 32097,  6200,   159,\n",
       "         32096,  6200,   159, 32095,  6200,   159, 32094,  6200,   159, 32093],\n",
       "        [    0, 32099,  6200,   159, 32098,  6200,   159, 32097,  6200,   159,\n",
       "             3,     5,     3,     5, 32096,  6200,   159, 32095,  6200,   159],\n",
       "        [    0, 32099,  6200,   159, 32098,  6200,   159,     3,     5,     3,\n",
       "             5, 32097,  6200,   159, 32096,  6200,   159, 32095,  6200,   159],\n",
       "        [    0, 32099,  6200,   159, 32098,  6200,   159, 32097,  6200,   159,\n",
       "         32096,  6200,   159,     3,     5,     3,     5, 32095,  6200,   159],\n",
       "        [    0, 32099,  6200,   159, 32098,  6200,   159, 32097,  6200,   159,\n",
       "         32096,  6200,   159, 32095,  6200,   159,     3,     5,     3,     5]])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# data = {'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"],\n",
    "#         'Age': [\"56\", \"45\", \"59\"],\n",
    "#         'Number of movies': [\"87\", \"53\", \"69\"]\n",
    "# }\n",
    "# table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# inputs = tokenizer(\n",
    "#         \"Actors Brad Pitt Age 56 Number of movies 87\", return_tensors='pt').input_ids\n",
    "# decoder_inputs_ids = tokenizer(\n",
    "#         \"Brad Pitt is\", return_tensors='pt').input_ids\n",
    "\n",
    "# outputs = model(input_ids = inputs, decoder_inputs_ids = decoder_inputs_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "tokenizer.mask_token"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "tokenizer"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='t5-base', vocab_size=32100, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('comp0087': conda)"
  },
  "interpreter": {
   "hash": "6b9679e1c6c858221750b0560d352a46169916bc602dce44bbd2a48d631c8c5d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}